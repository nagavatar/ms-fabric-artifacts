{"cells":[{"cell_type":"code","source":["from notebookutils import mssparkutils\n","# mssparkutils.fs.help()\n","\n","# https://learn.microsoft.com/en-us/fabric/data-engineering/notebook-utilities # Documentation for notebookutils\n","# https://learn.microsoft.com/en-us/fabric/data-engineering/microsoft-spark-utilities # Documentation for mssparkutils"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":40,"statement_ids":[40],"state":"finished","livy_statement_state":"available","session_id":"f9ccb846-27f8-4d97-b462-6ead819afb15","normalized_state":"finished","queued_time":"2025-02-10T11:13:00.4021741Z","session_start_time":null,"execution_start_time":"2025-02-10T11:13:00.5243671Z","execution_finish_time":"2025-02-10T11:13:00.7819371Z","parent_msg_id":"d4d5b2d0-f5d7-4a49-a6c4-cbe563bc72b2"},"text/plain":"StatementMeta(, f9ccb846-27f8-4d97-b462-6ead819afb15, 40, Finished, Available, Finished)"},"metadata":{}}],"execution_count":38,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"e77b599d-e465-437d-923d-ad31bd668ce6"},{"cell_type":"markdown","source":["# List all file and folders"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"bebe8fe9-4669-4601-99c2-64647167a0ba"},{"cell_type":"code","source":["files = mssparkutils.fs.ls(\"Files/\")\n","for file in files:\n","    print(f\"Name: {file.name}, Size: {file.size}, Type: {'Folder' if file.isDir else 'File'}\")\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":23,"statement_ids":[23],"state":"finished","livy_statement_state":"available","session_id":"f9ccb846-27f8-4d97-b462-6ead819afb15","normalized_state":"finished","queued_time":"2025-02-10T10:56:38.3011889Z","session_start_time":null,"execution_start_time":"2025-02-10T10:56:38.4744473Z","execution_finish_time":"2025-02-10T10:56:38.7998415Z","parent_msg_id":"cda48fbc-a62a-4d90-b4c9-8c4b4931f244"},"text/plain":"StatementMeta(, f9ccb846-27f8-4d97-b462-6ead819afb15, 23, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Name: 1_calendar_copy.csv, Size: 698286, Type: File\nName: calendar.csv, Size: 698286, Type: File\nName: calendar_copy.csv, Size: 698286, Type: File\nName: test, Size: 0, Type: Folder\n"]}],"execution_count":21,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c0fbf13c-360c-4ea3-a5c1-e956f0af9e50"},{"cell_type":"markdown","source":["# Approach 1: Read files one by one"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"52677065-bf70-4692-8588-37332dac0f2f"},{"cell_type":"code","source":["# Define the Lakehouse path\n","lakehouse_path = \"Files/\"  # Update to your folder containing files\n","\n","# List all files in the Lakehouse directory\n","files = mssparkutils.fs.ls(lakehouse_path)\n","\n","# Process files one by one\n","for file in files:\n","    if not file.isDir:  # Process only files, skip directories\n","        file_path = f\"{lakehouse_path}/{file.name}\"\n","        print(f\"Processing file: {file_path}\")\n","\n","        # Read the file into a DataFrame\n","        df = spark.read.format(\"csv\").option(\"header\", \"true\").load(file_path)  # Change format if needed\n","\n","        # Append data to Delta table\n","        df.write.format(\"delta\").mode(\"append\").saveAsTable('Test')\n","\n","print(f\"All files appended to Delta table\")\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":11,"statement_ids":[11],"state":"finished","livy_statement_state":"available","session_id":"f9ccb846-27f8-4d97-b462-6ead819afb15","normalized_state":"finished","queued_time":"2025-02-10T10:47:18.0987823Z","session_start_time":null,"execution_start_time":"2025-02-10T10:47:18.2509499Z","execution_finish_time":"2025-02-10T10:47:39.7088341Z","parent_msg_id":"e7b07e59-9a2f-44e9-a258-a2eba53da804"},"text/plain":"StatementMeta(, f9ccb846-27f8-4d97-b462-6ead819afb15, 11, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Processing file: Files//calendar.csv\nProcessing file: Files//calendar_copy.csv\nAll files appended to Delta table at: Files/delta_table/\n"]}],"execution_count":9,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"50cd7a09-69d4-433e-a7ee-41f742976aa6"},{"cell_type":"markdown","source":["# Approach 2: Read all files and then populate them together to delta table"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"53d7fd71-6e8a-4ec8-a47f-e81a6b5253fa"},{"cell_type":"code","source":["# Define the Lakehouse path\n","lakehouse_path = \"Files/\"  # Update to your folder containing files\n","\n","# List all files in the Lakehouse directory\n","files = mssparkutils.fs.ls(lakehouse_path)\n","\n","# Collect file paths into a list\n","file_paths = [f\"{lakehouse_path}/{file.name}\" for file in files if not file.isDir]\n","\n","# Read all files into a single DataFrame\n","df = spark.read.format(\"csv\").option(\"header\", \"true\").load(file_paths)  # Change format if needed\n","\n","# Write the DataFrame to the Delta table\n","df.write.format(\"delta\").mode(\"append\").saveAsTable('test')\n","\n","print(f\"All files combined and written to Delta table\")\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":19,"statement_ids":[19],"state":"finished","livy_statement_state":"available","session_id":"f9ccb846-27f8-4d97-b462-6ead819afb15","normalized_state":"finished","queued_time":"2025-02-10T10:51:55.8484821Z","session_start_time":null,"execution_start_time":"2025-02-10T10:51:56.0033517Z","execution_finish_time":"2025-02-10T10:51:57.4968238Z","parent_msg_id":"0f3e1a27-1f50-4520-9240-b7fa240c1eea"},"text/plain":"StatementMeta(, f9ccb846-27f8-4d97-b462-6ead819afb15, 19, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["All files combined and written to Delta table\n"]}],"execution_count":17,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"745d4505-60fd-48a3-8948-aeaa501bb73e"},{"cell_type":"markdown","source":["# Validate the counts"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"559cb51a-86a5-4cbb-b5a8-f58bae56c355"},{"cell_type":"code","source":["df = spark.sql(\"SELECT * FROM DE_LH_200_LAKE_DevOps_API.dbo.test\")\n","df.count()"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":20,"statement_ids":[20],"state":"finished","livy_statement_state":"available","session_id":"f9ccb846-27f8-4d97-b462-6ead819afb15","normalized_state":"finished","queued_time":"2025-02-10T10:51:56.3388946Z","session_start_time":null,"execution_start_time":"2025-02-10T10:51:57.619689Z","execution_finish_time":"2025-02-10T10:51:59.1046669Z","parent_msg_id":"b835f2ed-8fc7-482f-969e-703c4d475b4c"},"text/plain":"StatementMeta(, f9ccb846-27f8-4d97-b462-6ead819afb15, 20, Finished, Available, Finished)"},"metadata":{}},{"output_type":"execute_result","execution_count":59,"data":{"text/plain":"102272"},"metadata":{}}],"execution_count":18,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"49d96bb3-3f23-42bf-a37c-682ade9cab08"},{"cell_type":"markdown","source":["# Approach 1: Read Files One by One Based on Regex"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"9234c0ba-f058-473c-b5f0-64326b283466"},{"cell_type":"code","source":["import re\n","\n","# Define the Lakehouse path\n","lakehouse_path = \"Files/\"  # Update to your folder containing files\n","\n","# Regex pattern to match files starting with a letter\n","regex_pattern = r\"^[a-zA-Z].*\"\n","\n","# List all files in the Lakehouse directory\n","files = mssparkutils.fs.ls(lakehouse_path)\n","\n","# Process files one by one\n","for file in files:\n","    if not file.isDir and re.match(regex_pattern, file.name):  # Check regex for file name\n","        file_path = f\"{lakehouse_path}/{file.name}\"\n","        print(f\"Processing file: {file_path}\")\n","\n","        # Read the file into a DataFrame\n","        df = spark.read.format(\"csv\").option(\"header\", \"true\").load(file_path)  # Change format if needed\n","\n","        # Append data to Delta table\n","        df.write.format(\"delta\").mode(\"append\").saveAsTable('test_regex')\n","\n","print(f\"All matching files appended to Delta table\")\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":37,"statement_ids":[37],"state":"finished","livy_statement_state":"available","session_id":"f9ccb846-27f8-4d97-b462-6ead819afb15","normalized_state":"finished","queued_time":"2025-02-10T11:12:01.7986858Z","session_start_time":null,"execution_start_time":"2025-02-10T11:12:01.9132677Z","execution_finish_time":"2025-02-10T11:12:08.7996922Z","parent_msg_id":"bb14e0db-8fa5-4205-a5aa-6af746dfbb68"},"text/plain":"StatementMeta(, f9ccb846-27f8-4d97-b462-6ead819afb15, 37, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Processing file: Files//calendar.csv\nProcessing file: Files//calendar_copy.csv\nAll matching files appended to Delta table\n"]}],"execution_count":35,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"e431ba97-43a3-4f34-9ae5-84c81281bb3b"},{"cell_type":"markdown","source":["# Approach 2: Read All Matching Files Together Based on Regex"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"37309e70-76c3-4d52-9635-70469ba94999"},{"cell_type":"code","source":["import re\n","\n","# Define the Lakehouse path\n","lakehouse_path = \"Files/\"  # Update to your folder containing files\n","\n","# Regex pattern to match files starting with a letter\n","regex_pattern = r\"^[a-zA-Z].*\"\n","\n","# List all files in the Lakehouse directory\n","files = mssparkutils.fs.ls(lakehouse_path)\n","\n","# Collect paths of files matching the regex and print matching file names\n","file_paths = []\n","for file in files:\n","    if not file.isDir and re.match(regex_pattern, file.name):  # Check regex for file name\n","        print(f\"Matching file: {file.name}\")  # Print file name\n","        file_paths.append(f\"{lakehouse_path}/{file.name}\")\n","\n","# Check if any files matched the criteria\n","if not file_paths:\n","    print(\"No files matched the regex pattern.\")\n","else:\n","    # Read all matching files into a single DataFrame\n","    print(f\"Processing files: {file_paths}\")\n","    df = spark.read.format(\"csv\").option(\"header\", \"true\").load(file_paths)  # Change format if needed\n","\n","    # Write the DataFrame to the Delta table\n","    df.write.format(\"delta\").mode(\"overwrite\").saveAsTable('test_regex')\n","\n","    print(f\"All matching files combined and written to Delta table\")\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":33,"statement_ids":[33],"state":"finished","livy_statement_state":"available","session_id":"f9ccb846-27f8-4d97-b462-6ead819afb15","normalized_state":"finished","queued_time":"2025-02-10T11:11:16.3267929Z","session_start_time":null,"execution_start_time":"2025-02-10T11:11:16.4627509Z","execution_finish_time":"2025-02-10T11:11:20.031649Z","parent_msg_id":"5ab47079-c3d9-455f-b3a6-8b2c4503f164"},"text/plain":"StatementMeta(, f9ccb846-27f8-4d97-b462-6ead819afb15, 33, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Matching file: calendar.csv\nMatching file: calendar_copy.csv\nProcessing files: ['Files//calendar.csv', 'Files//calendar_copy.csv']\nAll matching files combined and written to Delta table\n"]}],"execution_count":31,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4d5bb801-d502-4c2b-8254-310b3fc646b6"},{"cell_type":"code","source":["df = spark.sql(\"SELECT * FROM DE_LH_200_LAKE_DevOps_API.dbo.test_regex\")\n","df.count()"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":38,"statement_ids":[38],"state":"finished","livy_statement_state":"available","session_id":"f9ccb846-27f8-4d97-b462-6ead819afb15","normalized_state":"finished","queued_time":"2025-02-10T11:12:03.9236706Z","session_start_time":null,"execution_start_time":"2025-02-10T11:12:08.9412382Z","execution_finish_time":"2025-02-10T11:12:11.3137149Z","parent_msg_id":"80edb836-0a16-4ed8-921e-b5eaa8343bc0"},"text/plain":"StatementMeta(, f9ccb846-27f8-4d97-b462-6ead819afb15, 38, Finished, Available, Finished)"},"metadata":{}},{"output_type":"execute_result","execution_count":113,"data":{"text/plain":"153408"},"metadata":{}}],"execution_count":36,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"58eaab9b-4170-494d-8db6-4020297f6537"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"default_lakehouse":"c1333e44-91f3-46d5-8f7b-c35e2c48fb80","default_lakehouse_name":"DE_LH_200_LAKE_DevOps_API","default_lakehouse_workspace_id":"a8e99a0f-30e6-42f8-a8a2-841e3f486f2e"}}},"nbformat":4,"nbformat_minor":5}